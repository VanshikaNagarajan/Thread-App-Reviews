{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1284fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('threads_trained_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump('svm_model', model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07ae63c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vanshika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vanshika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f370f871",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m model_file:\n\u001b[1;32m      4\u001b[0m     svm_model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(model_file)\n\u001b[0;32m----> 6\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(svm_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_description\u001b[39m\u001b[38;5;124m'\u001b[39m], svm_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m )\n\u001b[1;32m      7\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m)\n\u001b[1;32m      8\u001b[0m x_train_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(x_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_path = '/Users/vanshika/PROJECTS/threads_app_reviews/threads_trained_model.pkl'\n",
    "with open(model_path, 'rb') as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(svm_model['review_description'], svm_model['sentiment'], test_size=0.2 )\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=4000)\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(x_train_tfidf, y_train)\n",
    "svm_predictions = svm_model.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe1b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def refine_text(text):\n",
    "#    token = word_tokenize(text)\n",
    "#    token = [word.lower() for word in token if word.isalnum()]\n",
    "#    stop_words = set(stopwords.words('english'))\n",
    "#    token = [word for word in token if word not in stop_words]\n",
    "#    refine_text = ' '.join(token)\n",
    "#    return refine_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebe9ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review = input(\"Please give your review: \")\n",
    "#refined_review = refine_text(review)\n",
    "#review_tfidf = tfidf_vectorizer.transform([refined_review])\n",
    "#predicted_sentiment = svm_model.predict(review_tfidf)[0]\n",
    "#print(\"Predicted Sentiment for Your Review: \", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48f0723a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de089ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your review: the app is very bad\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide your review: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Predict sentiment\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sentiment_prediction \u001b[38;5;241m=\u001b[39m predict_sentiment(user_input, svm_model, tfidf_vectorizer)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Sentiment: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sentiment_prediction)\n",
      "Cell \u001b[0;32mIn[38], line 14\u001b[0m, in \u001b[0;36mpredict_sentiment\u001b[0;34m(input_text, svm_model, tfidf_vectorizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m preprocessed_text \u001b[38;5;241m=\u001b[39m preprocess_text(input_text)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Transform the preprocessed text into a TF-IDF vector\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m input_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform([preprocessed_text])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Predict the sentiment\u001b[39;00m\n\u001b[1;32m     17\u001b[0m predicted_sentiment \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(input_tfidf)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2155\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents):\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \n\u001b[1;32m   2142\u001b[0m \u001b[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2157\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead1048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
